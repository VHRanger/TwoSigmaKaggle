{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# widen jupyter display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#### in a kaggle kernel ####\n",
    "# from kaggle.competitions import twosigmanews\n",
    "# env = twosigmanews.make_env()\n",
    "# train = env.get_training_data()\n",
    "# test = []\n",
    "# for (market_obs, news_obs, predictions_template) in env.get_prediction_days():\n",
    "#     test.append((market_obs, news_obs, predictions_template))\n",
    "#     predictions_template.confidenceValue = 0.0\n",
    "#     env.predict(predictions_template)\n",
    "\n",
    "#### markettuple is a tuple\n",
    "#### ((train_mkt, train_news), [(test_mkt, test_news, predictions_template)])\n",
    "#### Where train is 2008 - 2016 and test is 2017 -\n",
    "# markettuple = (train, test)\n",
    "\n",
    "#### to dump on kaggle kernel to local machine\n",
    "### Data can be downloaded here\n",
    "### https://www.kaggleusercontent.com/kf/8462790/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..0tg-orjoMRCx0BGA2qXftw.OSv99PzfpWWJwCEVQYq5o-i3F7kS_zkCI3G3fVRK502-Jc8n2I93M9YCym8vlh9bJP50Pp1N1BaxDVwJsr1N35hXjzWD7bv6dHRm0MGCdgNsQfP4cJIy0krnO3gvCKYZSDirp3Ux1AeSUU4eV472nw.FMlRZVHOsCsVtzo4aBPGvA/data.p\n",
    "### From here\n",
    "### https://www.kaggle.com/vodkahaze/data-download/output\n",
    "# pickle.dump(markettuple, open('data.p', 'wb'))\n",
    "\n",
    "#### On local machine ####\n",
    "markettuple = pickle.load(open('data.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLED_STOCKS = 850 # Sample on n stocks\n",
    "NEWS_SAMPLE = 1_000_000\n",
    "TEST_DAYS = 183\n",
    "EARLIEST_DAY = '2009-01-01'\n",
    "\n",
    "# NN params\n",
    "BATCH_SIZE = 10000\n",
    "LOOK_BACK = 30\n",
    "LOOK_BACK_STEPS = 5\n",
    "EPOCHS = 20\n",
    "STEPS_PER_EPOCHS = 50\n",
    "VALIDATION_STEPS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkt = markettuple[0][0].loc[\n",
    "    markettuple[0][0].time >= EARLIEST_DAY].copy()\n",
    "if SAMPLED_STOCKS < mkt.assetCode.nunique():\n",
    "    accountsample = list(mkt.assetCode.sample(SAMPLED_STOCKS).unique())\n",
    "    mkt = mkt.loc[mkt.assetCode.isin(accountsample)]\n",
    "nw = markettuple[0][1].loc[\n",
    "    markettuple[0][1].time >= EARLIEST_DAY].copy()\n",
    "if NEWS_SAMPLE < len(nw):\n",
    "    nw = nw.sample(NEWS_SAMPLE)\n",
    "# news dataset has assetCodes in\n",
    "#   \"{'asset1', 'asset2', 'asset3'}\" format\n",
    "# Can select on this using a regex match\n",
    "if SAMPLED_STOCKS < mkt.assetCode.nunique():\n",
    "    patt = \"|\".join([\"{}\".format(i[:-2]) for i in accountsample])\n",
    "    nw = nw.loc[nw.assetCodes.str.contains(patt, regex=True, case=False)]\n",
    "\n",
    "markettuple = None\n",
    "gc.collect()\n",
    "mkt = mkt.sort_values(by='time'\n",
    "        ).reset_index(drop=True)\n",
    "nw = nw.sort_values(by='time'\n",
    "      ).reset_index(drop=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def market_prepro(\n",
    "    df_mkt, \n",
    "    label_col = \"returnsOpenNextMktres10\",\n",
    "    clean_data=True, \n",
    "    extract_time_info=True,\n",
    "    scale_numeric=False,\n",
    "    encode_asset=True,\n",
    "    fillna_clean_outliers=True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Preprocess market data\n",
    "    \"\"\"\n",
    "    df = df_mkt.copy()\n",
    "    # numeric columns\n",
    "    ncols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', \n",
    "             'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', \n",
    "             'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n",
    "    if scale_numeric is True and fillna_clean_outliers is False:\n",
    "        raise ValueError(\"scale_numeric can't be done without fillna\")\n",
    "    # Clean bad data. We fit on train dataset and it's ok to remove bad data\n",
    "    # Remove strange cases with close/open ratio > 2\n",
    "    if clean_data:\n",
    "        max_ratio  = 2\n",
    "        df = df[(df['close'] / df['open']).abs() <= max_ratio].loc[:]\n",
    "        df = df.reset_index(drop=True)\n",
    "    # Fill na, fix outliers. Safe for test dataset, no rows removed.\n",
    "    if fillna_clean_outliers:\n",
    "        # Fill nans\n",
    "        df[ncols] = df[['assetCode'] + ncols\n",
    "                                  ].groupby('assetCode'\n",
    "                                  ).transform(lambda g: g.fillna(method='bfill'))\n",
    "        df[ncols] = df[ncols].fillna(0)\n",
    "        # Fix outliers\n",
    "        df[ncols] = df[ncols].clip(\n",
    "                        df[ncols].quantile(0.01), \n",
    "                        df[ncols].quantile(0.99), \n",
    "                        axis=1)\n",
    "    # Extract day, week, year from time\n",
    "    if extract_time_info:\n",
    "        df = df.join(pd.get_dummies(\n",
    "            df.time.dt.year, prefix=\"year\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "        df = df.join(pd.get_dummies(\n",
    "            df.time.dt.day, prefix=\"day\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "        df = df.join(pd.get_dummies(\n",
    "            df.time.dt.week, prefix=\"week\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "        df = df.join(pd.get_dummies(\n",
    "            df.time.dt.dayofweek, prefix=\"dayofweek\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "        # Create linear time index column\n",
    "        udays = sorted(list(df.time.unique()))\n",
    "        timeindexdf = pd.DataFrame(np.arange(len(udays)), \n",
    "                                   index=udays\n",
    "                                  ).reset_index(drop=False)\n",
    "        timeindexdf.columns = ['time', 'timeIndex']\n",
    "        df = pd.merge(df, timeindexdf, how='left', on='time')\n",
    "    if encode_asset:\n",
    "        df = df.join(pd.get_dummies(\n",
    "                df['assetCode'], prefix=\"assetCode\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "    if scale_numeric:\n",
    "        # Fit for numeric and time\n",
    "        df[ncols] = StandardScaler().fit_transform(\n",
    "                                df[ncols].astype(float))\n",
    "    feature_cols = list(df.columns)\n",
    "    return df[feature_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_prepro(\n",
    "    df_news,\n",
    "    scale_numeric = True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Aggregate news by day and asset. Normalize numeric values.\n",
    "    Prepare news batch for generator.\n",
    "    Asset can have many news per day, so group them by asset, day and aggregate. \n",
    "    Then normalize numerical values. News aggregation part is based on this kernel: \n",
    "    https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data#\n",
    "    \"\"\"\n",
    "    news_cols_agg = {\n",
    "        'urgency': ['min', 'count'],\n",
    "        'takeSequence': ['max'],\n",
    "        'bodySize': ['min', 'max', 'mean', 'std'],\n",
    "        'wordCount': ['min', 'max', 'mean', 'std'],\n",
    "        'sentenceCount': ['min', 'max', 'mean', 'std'],\n",
    "        'companyCount': ['min', 'max', 'mean', 'std'],\n",
    "        'marketCommentary': ['min', 'max', 'mean', 'std'],\n",
    "        'relevance': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentNegative': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentPositive': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts7D': ['min', 'max', 'mean', 'std'],\n",
    "    }\n",
    "    news_cols_numeric = set(news_cols_agg.keys()) - set(['assetCode', 'time'])\n",
    "    # Fill na with previous value for the asset\n",
    "    df = df_news.copy()\n",
    "    # Aggregate by time, asset code\n",
    "    # Fix asset codes (str -> list)\n",
    "    # Since asset codes are in {1, 2, 3} format\n",
    "    # We need to repeat rows for each asset code\n",
    "    df['assetCodes'] = df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n",
    "    # Group to date level\n",
    "    df.time = df.time.astype('datetime64[D, UTC]')\n",
    "    # Expand assetCodes\n",
    "    assetCodes_expanded = list(itertools.chain(*df['assetCodes']))\n",
    "    if not df.empty: \n",
    "        assetCodes_index = df.index.repeat(df['assetCodes'].apply(len)) \n",
    "    else: assetCodes_index = df.index\n",
    "    assert len(assetCodes_index) == len(assetCodes_expanded)\n",
    "    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, \n",
    "                                  'assetCode': assetCodes_expanded})\n",
    "    # Create expanded news (will repeat every assetCodes' row)\n",
    "    news_cols = ['time', 'assetCodes'] + sorted(list(news_cols_agg.keys()))\n",
    "    df_expanded = pd.merge(df_assetCodes, df[news_cols],\n",
    "                           left_on='level_0', right_index=True, \n",
    "                           suffixes=(['','_old']))\n",
    "    # Aggregate numerical news features\n",
    "    df_agg = df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n",
    "    # Flat columns\n",
    "    df_agg.columns = ['_'.join(col).strip() for col in df_agg.columns.values]\n",
    "    # Normalize, fillna etc. Don't remove rows.\n",
    "    df_agg = df_agg.fillna(0)\n",
    "    if not df_agg.empty:\n",
    "        news_df_numeric = df_agg._get_numeric_data().astype(float)\n",
    "        df_agg[news_df_numeric.columns] = StandardScaler().fit_transform(news_df_numeric)\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joined_prepro(\n",
    "    mkt_df, nw_df,\n",
    "    label_col = \"returnsOpenNextMktres10\",\n",
    "    quant_features=False,\n",
    "    clean_data=False, \n",
    "    extract_time_info=True,\n",
    "    scale_numeric=False,\n",
    "    encode_asset=True,\n",
    "    fillna_clean_outliers=True,\n",
    "    time_shifts=0,\n",
    "    time_shift_cols=[\n",
    "        \"volume\", \"close\", \"open\",]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Returns X\n",
    "    \"\"\"\n",
    "    # market has index [time, assetCode]\n",
    "    dfmkt = market_prepro(mkt_df,\n",
    "        label_col=label_col,\n",
    "        clean_data=clean_data, \n",
    "        extract_time_info=extract_time_info,\n",
    "        scale_numeric=scale_numeric,\n",
    "        encode_asset=encode_asset,\n",
    "        fillna_clean_outliers=fillna_clean_outliers)\n",
    "    # We select news in train time interval\n",
    "    dfnews = news_prepro(nw_df,\n",
    "        scale_numeric=scale_numeric)\n",
    "    # Join by index, which is time, assetCode. \n",
    "    # Some assets have no news at all, so left join and 0 nans\n",
    "    X = dfmkt.merge(dfnews, how='left', left_on=['time', 'assetCode'], \n",
    "                    right_on=['time','assetCode'],  right_index=True)\n",
    "    if fillna_clean_outliers:\n",
    "        # Some market data can be without news, fill nans\n",
    "        X.loc[:, 'urgency_min':] = X.loc[:, 'urgency_min':].fillna(0.0)\n",
    "    # Drop non-training columns\n",
    "    y = X.returnsOpenNextMktres10.copy()\n",
    "    if time_shifts > 0:\n",
    "        X = X.sort_values(by=['assetCode', 'time']).reset_index(drop=True)\n",
    "        shifts = []\n",
    "        for col_name in time_shift_cols:\n",
    "            for i in range(1, time_shifts + 1):\n",
    "                shifts.append(X[[\"assetCode\", col_name]].groupby(\"assetCode\").shift(i))\n",
    "                shifts[-1].rename(lambda coln: coln + \"_L\" + str(i), \n",
    "                                  axis='columns',\n",
    "                                  inplace=True)\n",
    "        shifts = pd.concat(shifts, axis=1)\n",
    "        X = X.join(shifts)\n",
    "        X.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        gc.collect()\n",
    "    X = X.drop(['assetCode', 'assetName', label_col], axis=1)\n",
    "    gc.collect()\n",
    "    X = X.fillna(0.0)\n",
    "    gc.collect()\n",
    "    X = X.reset_index(drop=True)\n",
    "    gc.collect()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat Autoregressive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = joined_prepro(\n",
    "    mkt, nw,\n",
    "    label_col = \"returnsOpenNextMktres10\",\n",
    "    quant_features=False,\n",
    "    clean_data=False,\n",
    "    extract_time_info=True,\n",
    "    scale_numeric=False,\n",
    "    encode_asset=True,\n",
    "    fillna_clean_outliers=True,\n",
    "    time_shifts=48,\n",
    "    time_shift_cols=[\n",
    "        \"volume\",\n",
    "        \"close\", \n",
    "        \"open\",\n",
    "        \"returnsClosePrevRaw1\", \n",
    "        \"returnsOpenPrevRaw1\", \n",
    "        \"returnsClosePrevMktres1\", \n",
    "        \"returnsOpenPrevMktres1\", \n",
    "        \"returnsClosePrevRaw10\",\n",
    "        \"returnsOpenPrevRaw10\",\n",
    "        \"returnsClosePrevMktres10\", \n",
    "        \"returnsOpenPrevMktres10\",\n",
    "    ])\n",
    "\n",
    "latest_day = X.time.max()\n",
    "test_start = latest_day - datetime.timedelta(days=TEST_DAYS)\n",
    "\n",
    "X_test = X.loc[X.time >= test_start].copy()\n",
    "X = X.loc[X.time < test_start]\n",
    "gc.collect()\n",
    "y_test = y.loc[~y.index.isin(X.index)].copy()\n",
    "y = y.loc[y.index.isin(X.index)]\n",
    "gc.collect()\n",
    "\n",
    "mkt = None\n",
    "nw = None\n",
    "gc.collect()\n",
    "\n",
    "X = X.drop(['time'], axis=1)\n",
    "\n",
    "# The index of the columns of the asset codes\n",
    "# used to generate batches by asset codes\n",
    "asset_code_column_indices = [\n",
    "    i for i, x in enumerate(X.columns) \n",
    "    if \"assetcode_\" in x.lower()\n",
    "]\n",
    "\n",
    "X = X.values\n",
    "y = y.values * 100\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values * 100\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_by_stock(X, y, asset_idx_list, batch_size):\n",
    "    \"\"\"\n",
    "    Generate batches by sampling from individual stocks \n",
    "    \n",
    "    NOTE:\n",
    "        BATCH SIZE IS CURRENTLY UNUSED. could be used to sample from the samples\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # random asset\n",
    "        _asset = np.random.choice(asset_idx_list)\n",
    "        # Get row idx for where the asset one hot encoded feature is true \n",
    "        _loc = X[:, _asset] > 0\n",
    "        batch_features = X[_loc]\n",
    "        batch_labels = y[_loc]\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LSTM, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true-y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return 1 - (SS_res / (SS_tot + K.epsilon()))\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='mean_squared_error', patience=5, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer='adam', \n",
    "              metrics=[r2_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "712/712 [==============================] - 36s 51ms/step - loss: nan - r2_score: nan\n",
      "Epoch 2/10\n",
      "  3/712 [..............................] - ETA: 33s - loss: nan - r2_score: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:535: RuntimeWarning: Early stopping conditioned on metric `mean_squared_error` which is not available. Available metrics are: loss,r2_score\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 34s 48ms/step - loss: nan - r2_score: nan\n",
      "Epoch 3/10\n",
      "712/712 [==============================] - 32s 45ms/step - loss: nan - r2_score: nan\n",
      "Epoch 4/10\n",
      "  8/712 [..............................] - ETA: 29s - loss: nan - r2_score: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f2a601081dd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlystopper\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "   epochs=25,\n",
    "   batch_size=32, \n",
    "   verbose=1,\n",
    "   callbacks=[earlystopper])\n",
    "\n",
    "# model.fit_generator(\n",
    "#     generator=generate_by_stock(\n",
    "#         X, y, \n",
    "#         asset_idx_list=asset_code_column_indices, \n",
    "#         batch_size=BATCH_SIZE),\n",
    "#     steps_per_epoch=len(asset_code_column_indices), \n",
    "#     epochs=10,\n",
    "#     verbose=1, \n",
    "#     callbacks=[earlystopper]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Just making sure you don't train the LSTM needlessly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator:\n",
    "    \"\"\"\n",
    "    Keras standard approach to generage batches for model.fit_generator() call.\n",
    "    \"\"\"\n",
    "    def __init__(self, prepro, market, news, index_df):\n",
    "        \"\"\"\n",
    "        @param preprocessor: market and news join preprocessor\n",
    "        @param market: full loaded market df\n",
    "        @param news: full loaded news df\n",
    "        @param index_df: df with assetCode and time of train or validation market data. Batches will be taken from them.\n",
    "        \"\"\"\n",
    "        self.market = market\n",
    "        self.prepro = prepro\n",
    "        self.news = news\n",
    "        self.index_df = index_df\n",
    "        self.asset_codes = self.index_df['assetCode'].unique().tolist()\n",
    "\n",
    "    def flow_lstm(self, batch_size, is_train, look_back, look_back_step):\n",
    "        \"\"\"\n",
    "        Generates batch data for LSTM NN\n",
    "        Each cycle in a loop we yield a batch for one training step in epoch. \n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Get market indices of random assets, sorted by assetCode, time.\n",
    "            batch_index_df = self.get_random_assets_idx(batch_size)\n",
    "            # Get X, y data for this batch, containing market and news, but without look back yet\n",
    "            X, y = self.get_batch(batch_index_df, is_train)\n",
    "            # Add look back data to X, y\n",
    "            X, y = self.with_look_back(X,y,look_back,look_back_step)\n",
    "            yield X,y\n",
    "    \n",
    "    def get_random_assets_idx(self, batch_size):\n",
    "        \"\"\"\n",
    "        Get random asset and it's last market data indices.\n",
    "        Repeat for next asset until we reach batch_size.\n",
    "        \"\"\"\n",
    "        # Insert first asset\n",
    "        asset = np.random.choice(self.asset_codes)\n",
    "        asset_codes.remove(asset)\n",
    "        batch_index_df = self.index_df[self.index_df.assetCode == asset].tail(batch_size)\n",
    "        # Repeat until reach batch_size records\n",
    "        while (batch_index_df.index.size < batch_size) and (len(asset_codes) > 0):\n",
    "            asset = np.random.choice(asset_codes)\n",
    "            asset_codes.remove(asset)\n",
    "            asset_index_df = self.index_df[self.index_df.assetCode == asset].tail(batch_size - batch_index_df.index.size)\n",
    "            batch_index_df = pd.concat([batch_index_df, asset_index_df])\n",
    "        \n",
    "        return batch_index_df.sort_values(by=['assetCode', 'time'])\n",
    "            \n",
    "    def get_batch(self, batch_idx, is_train):\n",
    "        \"\"\"\n",
    "        Get batch of market-news data without lookback\n",
    "        \"\"\"\n",
    "        market_df = self.market.loc[batch_idx.index]\n",
    "        # Select subset of news for future merge by assetCode and time. \n",
    "        news_df = self.news.merge(batch_idx, on=['time'])\n",
    "        # Remove bad rows, clean the data. It's ok for train.\n",
    "        if is_train: \n",
    "            market_df, news_df = prepro.fix_train(market_df, news_df)\n",
    "        # Join market and news using preprocessor       \n",
    "        X = self.prepro.get_X(market_df, news_df)\n",
    "        y = self.prepro.get_y(market_df)\n",
    "        return(X, y)\n",
    "    \n",
    "    # convert an array of values into a dataset matrix\n",
    "    def with_look_back(self, X, y, look_back, look_back_step):\n",
    "        \"\"\"\n",
    "        Add look back window values to prepare dataset for LSTM\n",
    "        \"\"\"\n",
    "        X_processed, y_processed = [], []\n",
    "        # Fix last window in batch, can be not full\n",
    "        if look_back > len(X): \n",
    "            look_back = len(X)\n",
    "            look_back_step = min(look_back_step, look_back)\n",
    "            \n",
    "        for i in range(0,len(X)-look_back+1):\n",
    "            # Add lookback to X\n",
    "            x_window = X.values[i:(i+look_back):look_back_step, :]\n",
    "            X_processed.append(x_window)\n",
    "            # If input is X only, we'll not output y\n",
    "            if y is None: continue\n",
    "            # Add lookback to y\n",
    "            y_window = y.values[i+look_back-1, :]\n",
    "            y_processed.append(y_window)\n",
    "        # Return Xy for train/test or X for prediction\n",
    "        if(y is not None): return np.array(X_processed), np.array(y_processed)\n",
    "        else: return np.array(X_processed)\n",
    "\n",
    "    \n",
    "# Train data generator instance\n",
    "join_generator = LSTMGenerator(prepro, mkt, nw, mkt)\n",
    "\n",
    "# Validation data generator instance\n",
    "val_generator = LSTMGenerator(prepro, mkt, nw, mkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, LSTM, Embedding\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "# Add an input layer market + news\n",
    "input_size = len(market_prepro.feature_cols) + len(news_prepro.feature_cols)\n",
    "# input_shape=(timesteps, input features)\n",
    "model.add(LSTM(units=128, return_sequences=True, input_shape=(None,input_size)))\n",
    "model.add(LSTM(units=64, return_sequences=True ))\n",
    "model.add(LSTM(units=32, return_sequences=False))\n",
    "# Add an output layer \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_file='best_weights.h5'\n",
    "# We'll stop training if no improvement after some epochs\n",
    "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "# Low, avg and high scor training will be saved here\n",
    "# Save the best model during the traning\n",
    "checkpointer = ModelCheckpoint(\n",
    "    weights_file,\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.001)\n",
    "\n",
    "training = model.fit_generator(\n",
    "    join_generator.flow_lstm(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        is_train=True, \n",
    "        look_back=LOOK_BACK, \n",
    "        look_back_step=LOOK_BACK_STEPS),\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator.flow_lstm(\n",
    "            batch_size=BATCH_SIZE,\n",
    "            is_train=False, \n",
    "            look_back=LOOK_BACK, \n",
    "            look_back_step=LOOK_BACK_STEPS),\n",
    "    steps_per_epoch=STEPS_PER_EPOCHS,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    callbacks=[earlystopper, checkpointer, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
