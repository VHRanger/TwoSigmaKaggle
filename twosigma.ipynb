{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# widen jupyter display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#### in a kaggle kernel ####\n",
    "# from kaggle.competitions import twosigmanews\n",
    "# env = twosigmanews.make_env()\n",
    "# train = env.get_training_data()\n",
    "# test = []\n",
    "# for (market_obs, news_obs, predictions_template) in env.get_prediction_days():\n",
    "#     test.append((market_obs, news_obs, predictions_template))\n",
    "#     predictions_template.confidenceValue = 0.0\n",
    "#     env.predict(predictions_template)\n",
    "\n",
    "#### markettuple is a tuple\n",
    "#### ((train_mkt, train_news), [(test_mkt, test_news, predictions_template)])\n",
    "#### Where train is 2008 - 2016 and test is 2017 -\n",
    "# markettuple = (train, test)\n",
    "\n",
    "#### to dump on kaggle kernel to local machine\n",
    "# pickle.dump(markettuple, open('data.p', 'wb'))\n",
    "\n",
    "#### On local machine ####\n",
    "markettuple = pickle.load(open('data.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMPLING = 200_000 # None or 0 to turn off\n",
    "TEST_DAYS = 183\n",
    "\n",
    "# NN params\n",
    "BATCH_SIZE = 10000\n",
    "LOOK_BACK = 30\n",
    "LOOK_BACK_STEPS = 5\n",
    "EPOCHS = 20\n",
    "STEPS_PER_EPOCHS = 50\n",
    "VALIDATION_STEPS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_day = markettuple[0][0].time.max()\n",
    "test_start = latest_day - datetime.timedelta(days=TEST_DAYS)\n",
    "mkt = markettuple[0][0].copy()\n",
    "mkt_test = mkt.loc[mkt.time >= test_start]\n",
    "mkt = mkt.loc[mkt.time < test_start]\n",
    "if TRAIN_SAMPLING < len(mkt):\n",
    "    mkt = mkt.sample(TRAIN_SAMPLING)\n",
    "nw = markettuple[0][1].copy()\n",
    "nw_test = nw.loc[nw.time >= test_start]\n",
    "nw = nw.loc[nw.time < test_start]\n",
    "if TRAIN_SAMPLING < len(nw):\n",
    "    nw = nw.sample(TRAIN_SAMPLING)\n",
    "\n",
    "markettuple = None\n",
    "gc.collect()\n",
    "mkt = mkt.sort_values(by='time'\n",
    "        ).reset_index(drop=True)\n",
    "nw = nw.sort_values(by='time'\n",
    "      ).reset_index(drop=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def market_prepro(\n",
    "    df_mkt, \n",
    "    clean_data=True, \n",
    "    extract_time_info=True,\n",
    "    scale_numeric=False,\n",
    "    encode_asset=True,\n",
    "    fillna_clean_outliers=True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Preprocess market data\n",
    "    \"\"\"\n",
    "    df = df_mkt.copy()\n",
    "    label_col = 'returnsOpenNextMktres10'\n",
    "    # numeric columns\n",
    "    ncols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', \n",
    "             'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', \n",
    "             'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n",
    "    if scale_numeric is True and fillna_clean_outliers is False:\n",
    "        raise ValueError(\"scale_numeric can't be done without fillna\")\n",
    "    # Clean bad data. We fit on train dataset and it's ok to remove bad data\n",
    "    # Remove strange cases with close/open ratio > 2\n",
    "    if clean_data:\n",
    "        max_ratio  = 2\n",
    "        df = df[(df['close'] / df['open']).abs() <= max_ratio].loc[:]\n",
    "        df = df.reset_index(drop=True)\n",
    "    # Fill na, fix outliers. Safe for test dataset, no rows removed.\n",
    "    if fillna_clean_outliers:\n",
    "        # Fill nans\n",
    "        df[ncols] = df[['assetCode'] + ncols\n",
    "                                  ].groupby('assetCode'\n",
    "                                  ).transform(lambda g: g.fillna(method='bfill'))\n",
    "        df[ncols] = df[ncols].fillna(0)\n",
    "        # Fix outliers\n",
    "        df[ncols] = df[ncols].clip(\n",
    "                        df[ncols].quantile(0.01), \n",
    "                        df[ncols].quantile(0.99), \n",
    "                        axis=1)\n",
    "    # Extract day, week, year from time\n",
    "    if extract_time_info:\n",
    "        df = df.join(pd.get_dummies(\n",
    "            df.time.dt.year, prefix=\"year\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "        df = df.join(pd.get_dummies(\n",
    "            df.time.dt.day, prefix=\"day\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "        df = df.join(pd.get_dummies(\n",
    "            df.time.dt.week, prefix=\"week\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "        df = df.join(pd.get_dummies(\n",
    "            df.time.dt.dayofweek, prefix=\"dayofweek\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "        # Create linear time index column\n",
    "        udays = sorted(list(df.time.unique()))\n",
    "        timeindexdf = pd.DataFrame(np.arange(len(udays)), \n",
    "                                   index=udays\n",
    "                                  ).reset_index(drop=False)\n",
    "        timeindexdf.columns = ['time', 'timeIndex']\n",
    "        df = pd.merge(df, timeindexdf, how='left', on='time')\n",
    "    if encode_asset:\n",
    "        df = df.join(pd.get_dummies(\n",
    "                df['assetCode'], prefix=\"assetCode\",\n",
    "                dummy_na=True, drop_first=True))\n",
    "    if scale_numeric:\n",
    "        # Fit for numeric and time\n",
    "        df[ncols] = StandardScaler().fit_transform(\n",
    "                                df[ncols].astype(float))\n",
    "    feature_cols = list(df.columns)\n",
    "    return df[feature_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_prepro(\n",
    "    df_news,\n",
    "    scale_numeric = True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Aggregate news by day and asset. Normalize numeric values.\n",
    "    Prepare news batch for generator.\n",
    "    Asset can have many news per day, so group them by asset, day and aggregate. \n",
    "    Then normalize numerical values. News aggregation part is based on this kernel: \n",
    "    https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data#\n",
    "    \"\"\"\n",
    "    news_cols_agg = {\n",
    "        'urgency': ['min', 'count'],\n",
    "        'takeSequence': ['max'],\n",
    "        'bodySize': ['min', 'max', 'mean', 'std'],\n",
    "        'wordCount': ['min', 'max', 'mean', 'std'],\n",
    "        'sentenceCount': ['min', 'max', 'mean', 'std'],\n",
    "        'companyCount': ['min', 'max', 'mean', 'std'],\n",
    "        'marketCommentary': ['min', 'max', 'mean', 'std'],\n",
    "        'relevance': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentNegative': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentPositive': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n",
    "        'volumeCounts7D': ['min', 'max', 'mean', 'std'],\n",
    "    }\n",
    "    news_cols_numeric = set(news_cols_agg.keys()) - set(['assetCode', 'time'])\n",
    "    # Fill na with previous value for the asset\n",
    "    df = df_news.copy()\n",
    "    # Aggregate by time, asset code\n",
    "    # Fix asset codes (str -> list)\n",
    "    # Since asset codes are in {1, 2, 3} format\n",
    "    # We need to repeat rows for each asset code\n",
    "    df['assetCodes'] = df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n",
    "    # Group to date level\n",
    "    df.time = df.time.astype('datetime64[D, UTC]')\n",
    "    # Expand assetCodes\n",
    "    assetCodes_expanded = list(itertools.chain(*df['assetCodes']))\n",
    "    if not df.empty: \n",
    "        assetCodes_index = df.index.repeat(df['assetCodes'].apply(len)) \n",
    "    else: assetCodes_index = df.index\n",
    "    assert len(assetCodes_index) == len(assetCodes_expanded)\n",
    "    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, \n",
    "                                  'assetCode': assetCodes_expanded})\n",
    "    # Create expanded news (will repeat every assetCodes' row)\n",
    "    news_cols = ['time', 'assetCodes'] + sorted(list(news_cols_agg.keys()))\n",
    "    df_expanded = pd.merge(df_assetCodes, df[news_cols],\n",
    "                           left_on='level_0', right_index=True, \n",
    "                           suffixes=(['','_old']))\n",
    "    # Aggregate numerical news features\n",
    "    df_agg = df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n",
    "    # Flat columns\n",
    "    df_agg.columns = ['_'.join(col).strip() for col in df_agg.columns.values]\n",
    "    # Normalize, fillna etc. Don't remove rows.\n",
    "    df_agg = df_agg.fillna(0)\n",
    "    if not df_agg.empty:\n",
    "        news_df_numeric = df_agg._get_numeric_data().astype(float)\n",
    "        df_agg[news_df_numeric.columns] = StandardScaler().fit_transform(news_df_numeric)\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewma = pd.Series.ewm\n",
    "def make_ewma(df,cols=['open',],n_roll=[12,26]):\n",
    "    for col in cols:\n",
    "        for n in n_roll:\n",
    "            df['{}_{}EMA'.format(col,n)] = ewma(df[col], span=n).mean()\n",
    "        for n1,n2 in itertools.combinations(n_roll,2):\n",
    "            df['{}_{}EMA-{}EMA'.format(col,n1,n2)] = df['{}_{}EMA'.format(col,n1)]- df['{}_{}EMA'.format(col,n2)]\n",
    "    return df\n",
    "\n",
    "def rsiFunc(prices, n=14):\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n+1]\n",
    "    up = seed[seed>=0].sum()/n\n",
    "    down = -seed[seed<0].sum()/n\n",
    "    rs = up/down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:n] = 100. - 100./(1.+rs)\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i-1] # cause the diff is 1 shorter\n",
    "        if delta>0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "        up = (up*(n-1) + upval)/n\n",
    "        down = (down*(n-1) + downval)/n\n",
    "        rs = up/down\n",
    "        rsi[i] = 100. - 100./(1.+rs)\n",
    "    return rsi\n",
    "\n",
    "def make_rsi(df,cols=['open',],n_roll=[30,]):\n",
    "    for col in cols:\n",
    "        for n in n_roll:\n",
    "            df['{}_{}RSI'.format(col,n)] = rsiFunc(df[col].values, n)\n",
    "    return df\n",
    "\n",
    "def add_quant_features(df,cols,n_ewma=[12,24],n_rsi=[30,]):\n",
    "    dfs = []\n",
    "    for i, df_group in df.groupby('assetCode'):\n",
    "        df_ = df_group.set_index('time').resample(\"1d\").ffill()\n",
    "        df_ = make_ewma(df_,cols=cols,n_roll=n_ewma)\n",
    "        df_ = make_rsi(df_,cols=cols,n_roll=n_rsi)\n",
    "        dfs.append(df_)\n",
    "        gc.collect()\n",
    "    gc.collect()\n",
    "    return pd.concat(dfs,axis=0,join='inner')\n",
    "\n",
    "def joined_prepro(\n",
    "    mkt_df, nw_df,\n",
    "    quant_features=False,\n",
    "    clean_data=False, \n",
    "    extract_time_info=True,\n",
    "    scale_numeric=False,\n",
    "    encode_asset=True,\n",
    "    fillna_clean_outliers=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Returns X\n",
    "    \"\"\"\n",
    "    # market has index [time, assetCode]\n",
    "    dfmkt = market_prepro(mkt_df,\n",
    "        clean_data=clean_data, \n",
    "        extract_time_info=extract_time_info,\n",
    "        scale_numeric=scale_numeric,\n",
    "        encode_asset=encode_asset,\n",
    "        fillna_clean_outliers=fillna_clean_outliers)\n",
    "    # We select news in train time interval\n",
    "    dfnews = news_prepro(nw_df,\n",
    "        scale_numeric=scale_numeric)\n",
    "    # Join by index, which is time, assetCode. \n",
    "    # Some assets have no news at all, so left join and 0 nans\n",
    "    X = dfmkt.merge(dfnews, how='left', left_on=['time', 'assetCode'], \n",
    "                    right_on=['time','assetCode'],  right_index=True)\n",
    "    if fillna_clean_outliers:\n",
    "        # Some market data can be without news, fill nans\n",
    "        X.loc[:, 'urgency_min':] = X.loc[:, 'urgency_min':].fillna(0.0)\n",
    "    # Drop non-training columns\n",
    "    y = X.returnsOpenNextMktres10.copy()\n",
    "    if quant_features:\n",
    "        X = add_quant_features(X, ['close'])\n",
    "    X = X.drop(['time', 'assetCode', 'assetName', 'returnsOpenNextMktres10'], axis=1)\n",
    "    X = X.fillna(0.0)\n",
    "    gc.collect()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = joined_prepro(\n",
    "    mkt,\n",
    "    nw,\n",
    "    quant_features=False,\n",
    "    clean_data=False,\n",
    "    extract_time_info=True,\n",
    "    scale_numeric=False,\n",
    "    encode_asset=True,\n",
    "    fillna_clean_outliers=True)\n",
    "\n",
    "X_test, y_test = joined_prepro(\n",
    "    mkt_test,\n",
    "    nw_test,\n",
    "    quant_features=False,\n",
    "    clean_data=False,\n",
    "    extract_time_info=True,\n",
    "    scale_numeric=True,\n",
    "    encode_asset=True,\n",
    "    fillna_clean_outliers=True)\n",
    "        \n",
    "X = X.values\n",
    "y = y.values\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LSTM, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true-y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return 1 - (SS_res / (SS_tot + K.epsilon()))\n",
    "\n",
    "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "\n",
    "def kmodel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[r2_score])\n",
    "    return model\n",
    "\n",
    "estimator = KerasRegressor(build_fn=kmodel, epochs=100,\n",
    "                           batch_size=32, verbose=1,\n",
    "                           callbacks=[earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Just making sure you don't train the LSTM needlessly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator:\n",
    "    \"\"\"\n",
    "    Keras standard approach to generage batches for model.fit_generator() call.\n",
    "    \"\"\"\n",
    "    def __init__(self, prepro, market, news, index_df):\n",
    "        \"\"\"\n",
    "        @param preprocessor: market and news join preprocessor\n",
    "        @param market: full loaded market df\n",
    "        @param news: full loaded news df\n",
    "        @param index_df: df with assetCode and time of train or validation market data. Batches will be taken from them.\n",
    "        \"\"\"\n",
    "        self.market = market\n",
    "        self.prepro = prepro\n",
    "        self.news = news\n",
    "        self.index_df = index_df\n",
    "        self.asset_codes = self.index_df['assetCode'].unique().tolist()\n",
    "\n",
    "    def flow_lstm(self, batch_size, is_train, look_back, look_back_step):\n",
    "        \"\"\"\n",
    "        Generates batch data for LSTM NN\n",
    "        Each cycle in a loop we yield a batch for one training step in epoch. \n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Get market indices of random assets, sorted by assetCode, time.\n",
    "            batch_index_df = self.get_random_assets_idx(batch_size)\n",
    "            # Get X, y data for this batch, containing market and news, but without look back yet\n",
    "            X, y = self.get_batch(batch_index_df, is_train)\n",
    "            # Add look back data to X, y\n",
    "            X, y = self.with_look_back(X,y,look_back,look_back_step)\n",
    "            yield X,y\n",
    "    \n",
    "    def get_random_assets_idx(self, batch_size):\n",
    "        \"\"\"\n",
    "        Get random asset and it's last market data indices.\n",
    "        Repeat for next asset until we reach batch_size.\n",
    "        \"\"\"\n",
    "        # Insert first asset\n",
    "        asset = np.random.choice(self.asset_codes)\n",
    "        asset_codes.remove(asset)\n",
    "        batch_index_df = self.index_df[self.index_df.assetCode == asset].tail(batch_size)\n",
    "        # Repeat until reach batch_size records\n",
    "        while (batch_index_df.index.size < batch_size) and (len(asset_codes) > 0):\n",
    "            asset = np.random.choice(asset_codes)\n",
    "            asset_codes.remove(asset)\n",
    "            asset_index_df = self.index_df[self.index_df.assetCode == asset].tail(batch_size - batch_index_df.index.size)\n",
    "            batch_index_df = pd.concat([batch_index_df, asset_index_df])\n",
    "        \n",
    "        return batch_index_df.sort_values(by=['assetCode', 'time'])\n",
    "            \n",
    "    def get_batch(self, batch_idx, is_train):\n",
    "        \"\"\"\n",
    "        Get batch of market-news data without lookback\n",
    "        \"\"\"\n",
    "        market_df = self.market.loc[batch_idx.index]\n",
    "        # Select subset of news for future merge by assetCode and time. \n",
    "        news_df = self.news.merge(batch_idx, on=['time'])\n",
    "        # Remove bad rows, clean the data. It's ok for train.\n",
    "        if is_train: \n",
    "            market_df, news_df = prepro.fix_train(market_df, news_df)\n",
    "        # Join market and news using preprocessor       \n",
    "        X = self.prepro.get_X(market_df, news_df)\n",
    "        y = self.prepro.get_y(market_df)\n",
    "        return(X, y)\n",
    "    \n",
    "    # convert an array of values into a dataset matrix\n",
    "    def with_look_back(self, X, y, look_back, look_back_step):\n",
    "        \"\"\"\n",
    "        Add look back window values to prepare dataset for LSTM\n",
    "        \"\"\"\n",
    "        X_processed, y_processed = [], []\n",
    "        # Fix last window in batch, can be not full\n",
    "        if look_back > len(X): \n",
    "            look_back = len(X)\n",
    "            look_back_step = min(look_back_step, look_back)\n",
    "            \n",
    "        for i in range(0,len(X)-look_back+1):\n",
    "            # Add lookback to X\n",
    "            x_window = X.values[i:(i+look_back):look_back_step, :]\n",
    "            X_processed.append(x_window)\n",
    "            # If input is X only, we'll not output y\n",
    "            if y is None: continue\n",
    "            # Add lookback to y\n",
    "            y_window = y.values[i+look_back-1, :]\n",
    "            y_processed.append(y_window)\n",
    "        # Return Xy for train/test or X for prediction\n",
    "        if(y is not None): return np.array(X_processed), np.array(y_processed)\n",
    "        else: return np.array(X_processed)\n",
    "\n",
    "    \n",
    "# Train data generator instance\n",
    "join_generator = LSTMGenerator(prepro, mkt, nw, mkt)\n",
    "\n",
    "# Validation data generator instance\n",
    "val_generator = LSTMGenerator(prepro, mkt, nw, mkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, LSTM, Embedding\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "# Add an input layer market + news\n",
    "input_size = len(market_prepro.feature_cols) + len(news_prepro.feature_cols)\n",
    "# input_shape=(timesteps, input features)\n",
    "model.add(LSTM(units=128, return_sequences=True, input_shape=(None,input_size)))\n",
    "model.add(LSTM(units=64, return_sequences=True ))\n",
    "model.add(LSTM(units=32, return_sequences=False))\n",
    "# Add an output layer \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_file='best_weights.h5'\n",
    "# We'll stop training if no improvement after some epochs\n",
    "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "# Low, avg and high scor training will be saved here\n",
    "# Save the best model during the traning\n",
    "checkpointer = ModelCheckpoint(\n",
    "    weights_file,\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.001)\n",
    "\n",
    "training = model.fit_generator(\n",
    "    join_generator.flow_lstm(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        is_train=True, \n",
    "        look_back=LOOK_BACK, \n",
    "        look_back_step=LOOK_BACK_STEPS),\n",
    "        epochs=EPOCHS, \n",
    "        validation_data=val_generator.flow_lstm(\n",
    "            batch_size=BATCH_SIZE,\n",
    "            is_train=False, \n",
    "            look_back=LOOK_BACK, \n",
    "            look_back_step=LOOK_BACK_STEPS),\n",
    "    steps_per_epoch=STEPS_PER_EPOCHS,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    callbacks=[earlystopper, checkpointer, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
